Case 1: Biased Hiring Tool (Amazon)

Source of Bias: Training data based on historical male-dominated resumes reinforced gender bias.

Fixes:

Balance the dataset with gender-diverse resumes.

Remove gender-correlated features (e.g., women’s clubs).

Apply fairness constraints (e.g., demographic parity).

Fairness Metrics:

Disparate impact ratio (should be 0.8–1.25).

Equal opportunity difference (compare true positive rates).

Case 2: Facial Recognition in Policing

Ethical Risks:

Wrongful arrests from misidentification.

Privacy violations through surveillance.

Disproportionate targeting of minorities.

Policies for Responsible Deployment:

Independent audits for bias before use.

Restrict use to high-stakes scenarios with human oversight.

Transparency: public disclosure of system use and limitations.
